<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="" xml:lang="">
<head>
  <meta charset="utf-8" />
  <meta name="generator" content="pandoc" />
  <meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes" />
  <title>CVDLEng</title>
  <style>
    /* Default styles provided by pandoc.
    ** See https://pandoc.org/MANUAL.html#variables-for-html for config info.
    */
    html {
      color: #1a1a1a;
      background-color: #fdfdfd;
    }
    body {
      margin: 0 auto;
      max-width: 36em;
      padding-left: 50px;
      padding-right: 50px;
      padding-top: 50px;
      padding-bottom: 50px;
      hyphens: auto;
      overflow-wrap: break-word;
      text-rendering: optimizeLegibility;
      font-kerning: normal;
    }
    @media (max-width: 600px) {
      body {
        font-size: 0.9em;
        padding: 12px;
      }
      h1 {
        font-size: 1.8em;
      }
    }
    @media print {
      html {
        background-color: white;
      }
      body {
        background-color: transparent;
        color: black;
        font-size: 12pt;
      }
      p, h2, h3 {
        orphans: 3;
        widows: 3;
      }
      h2, h3, h4 {
        page-break-after: avoid;
      }
    }
    p {
      margin: 1em 0;
    }
    a {
      color: #1a1a1a;
    }
    a:visited {
      color: #1a1a1a;
    }
    img {
      max-width: 100%;
    }
    svg {
      height: auto;
      max-width: 100%;
    }
    h1, h2, h3, h4, h5, h6 {
      margin-top: 1.4em;
    }
    h5, h6 {
      font-size: 1em;
      font-style: italic;
    }
    h6 {
      font-weight: normal;
    }
    ol, ul {
      padding-left: 1.7em;
      margin-top: 1em;
    }
    li > ol, li > ul {
      margin-top: 0;
    }
    blockquote {
      margin: 1em 0 1em 1.7em;
      padding-left: 1em;
      border-left: 2px solid #e6e6e6;
      color: #606060;
    }
    code {
      font-family: Menlo, Monaco, Consolas, 'Lucida Console', monospace;
      font-size: 85%;
      margin: 0;
      hyphens: manual;
    }
    pre {
      margin: 1em 0;
      overflow: auto;
    }
    pre code {
      padding: 0;
      overflow: visible;
      overflow-wrap: normal;
    }
    .sourceCode {
     background-color: transparent;
     overflow: visible;
    }
    hr {
      border: none;
      border-top: 1px solid #1a1a1a;
      height: 1px;
      margin: 1em 0;
    }
    table {
      margin: 1em 0;
      border-collapse: collapse;
      width: 100%;
      overflow-x: auto;
      display: block;
      font-variant-numeric: lining-nums tabular-nums;
    }
    table caption {
      margin-bottom: 0.75em;
    }
    tbody {
      margin-top: 0.5em;
      border-top: 1px solid #1a1a1a;
      border-bottom: 1px solid #1a1a1a;
    }
    th {
      border-top: 1px solid #1a1a1a;
      padding: 0.25em 0.5em 0.25em 0.5em;
    }
    td {
      padding: 0.125em 0.5em 0.25em 0.5em;
    }
    header {
      margin-bottom: 4em;
      text-align: center;
    }
    #TOC li {
      list-style: none;
    }
    #TOC ul {
      padding-left: 1.3em;
    }
    #TOC > ul {
      padding-left: 0;
    }
    #TOC a:not(:hover) {
      text-decoration: none;
    }
    code{white-space: pre-wrap;}
    span.smallcaps{font-variant: small-caps;}
    div.columns{display: flex; gap: min(4vw, 1.5em);}
    div.column{flex: auto; overflow-x: auto;}
    div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
    /* The extra [class] is a hack that increases specificity enough to
       override a similar rule in reveal.js */
    ul.task-list[class]{list-style: none;}
    ul.task-list li input[type="checkbox"] {
      font-size: inherit;
      width: 0.8em;
      margin: 0 0.8em 0.2em -1.6em;
      vertical-align: middle;
    }
    .display.math{display: block; text-align: center; margin: 0.5rem auto;}
  </style>
</head>
<body>
<style>
  body {
    background-color: white;
    color: black;
    font-family: "Aptos", "Segoe UI", system-ui, -apple-system, "Helvetica Neue", Arial, sans-serif;
  }
  a {
    color: #0F4761;
  }
  h1, h2, h3, h4, h5, h6 {
    color: #0F4761;
    font-family: "Aptos", "Segoe UI", system-ui, -apple-system, "Helvetica Neue", Arial, sans-serif;
  }
  h2 {
    margin-top: 1.6em;
    margin-bottom: 0.3em;
  }

  h3 {
    margin-top: 0.5em;
    margin-bottom: 0.3em;
  }
</style>
<p align="center">
<img src="inverted_logo.png" alt="Description" width="150"><br> <span
style="font-size: 1.2em; font-weight: 600;">PrediX</span>
</p>
<h2 id="job-description-computer-vision-deep-learning-engineer">Job
Description: Computer Vision Deep Learning Engineer</h2>
<h3 id="position-overview">Position Overview</h3>
<p>We are seeking an experienced <strong>Computer Vision &amp; Deep
Learning Engineer</strong> to drive development of multimodal imaging
algorithms at <strong>PrediX</strong>, a precision‑oncology startup
building an AI‑driven “virtual biopsy” using radiology and digital
pathology. You will join a small, high‑impact R&amp;D team to research,
develop, and deploy SOTA models for multimodal learning and
retrieval.</p>
<p><strong>Location:</strong> Ramat‑Gan (Hybrid)</p>
<p><strong>Department:</strong> R&amp;D</p>
<p><strong>Reports to:</strong> CTO</p>
<p><strong>Start date:</strong> Immediate</p>
<div style="margin:  1.2em 0 -1em 0;">
<hr style="border: 0; border-top: 2px solid black; margin: 0;" />
</div>
<h2 id="key-responsibilities">Key Responsibilities</h2>
<h3 id="handson-research-and-development">Hands‑On Research and
Development</h3>
<ul>
<li>Design, develop, and optimize multimodal (MRI/CT-WSI) models:
contrastive learning, representation learning, and retrieval.</li>
<li>Build segmentation and ROI pipelines for radiology and digital
pathology data (preprocessing, normalization, and quality control).</li>
<li>Run controlled experiments, fine‑tune foundation backbones, analyze
errors, and improve robustness and generalization.</li>
<li>Establish clear metrics and benchmarks, build evaluation suites and
dashboards.</li>
</ul>
<h2 id="qualifications">Qualifications</h2>
<h3 id="educational-background">Educational Background</h3>
<ul>
<li>M.Sc. or Ph.D. in Computer Science, Electrical/Biomedical
Engineering, Data Science, or related field with focus on computer
vision / deep learning.</li>
</ul>
<h3 id="experience">Experience</h3>
<ul>
<li>3+ years hands‑on developing and shipping deep learning models
(medical imaging preferred).</li>
<li>Track record in high‑impact computer vision projects.</li>
<li>Familiarity with regulated environments is a plus.</li>
</ul>
<h3 id="technical-skills">Technical Skills</h3>
<ul>
<li>Strong PyTorch experience with CNN/ViT/transformer architectures and
segmentation.</li>
<li>Hands-on experience and ability to develop new models for
contrastive/self‑supervised learning, retrieval systems, and
evaluation.</li>
<li>Innovative ability to design and scale practical data pipelines for
medical imaging is advantageous.</li>
<li>Solid engineering: Python (C++ nice‑to‑have), version control,
containers, cloud infrastructure (AWS or similar), and basic MLOps.</li>
</ul>
<h3 id="soft-skills">Soft Skills</h3>
<ul>
<li>Team player with excellent collaboration skills.</li>
<li>Strong problem‑solving and ownership.</li>
<li>Clear written/oral communication.</li>
</ul>
<div style="margin:  1.2em 0 -1em 0;">
<hr style="border: 0; border-top: 2px solid black; margin: 0;" />
</div>
<h2 id="why-join-predix">Why Join PrediX?</h2>
<p>Join a mission‑driven team advancing non‑invasive cancer diagnostics.
Your work will shape core technology that aims to spare biopsies, speed
treatment decisions, and bring precision oncology to the clinic.</p>
<p><strong>Please send your CV to:
<a href="mailto:jobs@predix-ai.com">jobs@predix-ai.com</a></strong></p>
</body>
</html>
